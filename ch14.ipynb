{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": ["import heapq\nfrom collections import defaultdict\nimport pandas as pd\nimport matplotlib.pylab as plt\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules\nfrom surprise import Dataset, Reader, KNNBasic\nfrom surprise.model_selection import train_test_split\n\n# Load and preprocess data set\nfp_df = pd.read_csv('Faceplate.csv')\nfp_df.set_index('Transaction', inplace=True)\nprint(fp_df)\n# create frequent itemsets\nitemsets = apriori(fp_df, min_support=0.2,\nuse_colnames=True)\n# convert into rules\nrules = association_rules(itemsets, metric='confidence',\nmin_threshold=0.5)\nrules.sort_values(by=['lift'], ascending=False).head(6)\nprint(rules.sort_values(by=['lift'], ascending=False))\n\n# load dataset\nall_books_df = pd.read_csv('CharlesBookClub.csv')\n# create the binary incidence matrix\nignore = ['Seq#', 'ID#', 'Gender', 'M', 'R', 'F',\n'FirstPurch', 'Related Purchase',\n'Mcode', 'Rcode', 'Fcode', 'Yes_Florence',\n'No_Florence']\ncount_books = all_books_df.drop(columns=ignore)\ncount_books[count_books > 0] = 1\n# create frequent itemsets and rules\nitemsets = apriori(count_books, min_support=200/4000,\nuse_colnames=True)\nrules = association_rules(itemsets, metric='confidence',\nmin_threshold=0.5)\n# Display 25 rules with highest lift\nrules.sort_values(by=['lift'], ascending=False).head(25)\n\n\n\n\nimport random\nrandom.seed(0)\nnratings = 5000\nrandomData = pd.DataFrame({\n'itemID': [random.randint(0,99) for _ in\nrange(nratings)],\n'userID': [random.randint(0,999) for _ in\nrange(nratings)],\n'rating': [random.randint(1,5) for _ in\nrange(nratings)],\n})\ndef get_top_n(predictions, n=10):\n  # First map the predictions to each user.\n  byUser = defaultdict(list)\n  for p in predictions:\n    byUser[p.uid].append(p)\n    # For each user, reduce predictions to top-n\n    for uid, userPredictions in byUser.items():\n      byUser[uid] = heapq.nlargest(n, userPredictions,key=lambda p: p.est)\n      return byUser\n\n\n# Convert the data set into the format required by the surprise package\n# The columns must correspond to user id, item id, and ratings (in that order)\nreader = Reader(rating_scale=(1, 5))\ndata = Dataset.load_from_df(randomData[['userID', 'itemID',\n'rating']], reader)\n# Split into training and test set\ntrainset, testset = train_test_split(data, test_size=.25,\nrandom_state=1)\n## User-based filtering\n# compute cosine similarity between users\nsim_options = {'name': 'cosine', 'user_based': True}\nalgo = KNNBasic(sim_options=sim_options)\nalgo.fit(trainset)\n# predict ratings for all pairs (u, i) that are NOT in the training set.\npredictions = algo.test(testset)\n# Print the recommended items for each user\ntop_n = get_top_n(predictions, n=4)\nprint('Top-3 recommended items for each user')\nfor uid, user_ratings in list(top_n.items())[:5]:\n  print('User {}'.format(uid))\n  # for prediction in user_ratings:\n    # print(' Item {0.iid}l({0.est:.2f})'.format(predictions, end=''))\n\n\ntrainset = data.build_full_trainset()\nsim_options = {'name': 'cosine', 'user_based': False}\nalgo = KNNBasic(sim_options=sim_options)\nalgo.fit(trainset)\n# Predict rating for user 383 and item 7\nalgo.predict(383, 7)\n\n\n"]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}